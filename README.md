# NLP Study (CUAI)
CS224N: Natural Language Processing with Deep Learning

- YouTube Video Lectures ([Link](https://youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D&si=r8djSieatiCCatQV)) by Stanford (Spring 2024)
- Reading papers per week

## Our Activities

| No.  | Date        | Title                          | Notes                          |
|------|-------------|--------------------------------|--------------------------------|
| 1    | 03/17 | Lecture 1 - Intro and Word Vectors         | [L1](https://shorturl.at/ADy2y) |
|      |       | Lecture 2 - Word Vectors and Language Model| [L2](https://shorturl.at/Lgzi8) |
|      |       | Presentation-1 in Regular Session            | [Slides](https://shorturl.at/OwxX8)|
| 2    | 03/24 | Lecture 3 - Backpropagation & Neural Network | [L3](https://shorturl.at/t2CWH) |
|      |       | Lecture 4 - Dependency Parsing |[L4](https://shorturl.at/mC5qH) |
|      |       | [A Fast and Accurate Dependency Parser using Neural Networks](https://shorturl.at/h65Vi) | [PPT](https://shorturl.at/xtpUV) |
| 3    | 03/31 | Lecture 5 - Recurrent Neural Network | [L5](https://shorturl.at/6jVBc) |
|      |       | Lecture 6 - Sequence to Sequence Models | [L6&L7](https://shorturl.at/Yt8hP) |
|      |       | Lecture 7 - Attention & LLM Intro |  |
|      |       | [Sequence to Sequence Learning with Neural Networks](https://shorturl.at/84JB0) | [PPT](https://shorturl.at/nd9y5) |
|      |       | Presentation-2 in Regular Session            | [Slides](https://shorturl.at/AcK6c)|
| 4    | ~04/28 | Mid-term Exam  |                |
| 5    | 05/02 | Lecture 8 - Self-Attention & Transformers | Link |
|      |       | Lecture 9 - Pretraining | Link |
|      |       | [Attention Is All You Need](https://shorturl.at/wyOFy) | [PPT](https://shorturl.at/FQwmK) |
|      |       | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://rb.gy/k56507) | Link |
| 6    | 05/12 | [HEGEL: Hypergraph Transformer for Long Document Summarization](https://aclanthology.org/2022.emnlp-main.692.pdf)  | [PPT](https://shorturl.at/pT1ne)|
|      |       | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461) | [PPT](https://shorturl.at/Xr6dm) |
|      | 05/13 | Presentation-3 in Regular Session            | [Slides](https://shorturl.at/GA8x0)|
| 7    | 05/19 | [Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN](https://arxiv.org/pdf/2412.13795) | 양희원 |
|      |       | [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://shorturl.at/FikHb)  | 태아카 |
| 8    | 05/26 | -  | 민유안 |

---
### Study Members
1. 민유안	(소프트웨어학부)	
2. 정인혁	(소프트웨어학부)
3. 김지호	(미디어커뮤니케이션학부)	
4. 태아카	(소프트웨어학부)
5. 양희원	(AI학과)	
