# NLP Study (CUAI)
CS224N: Natural Language Processing with Deep Learning

YouTube Video Lectures ([Link](https://youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D&si=r8djSieatiCCatQV)) by Stanford (Spring 2024)

## Our Activities

| No.  | Date        | Title                          | Notes                          |
|------|-------------|--------------------------------|--------------------------------|
| 1    | 03/17 | Lecture 1 - Intro and Word Vectors         | [L1](https://shorturl.at/ADy2y) |
|      |       | Lecture 2 - Word Vectors and Language Model| [L2](https://shorturl.at/Lgzi8) |
|      |       | Presentation in Regular Session            | [Slides](https://shorturl.at/OwxX8)|
| 2    | 03/24 | Lecture 3 - Backpropagation & Neural Network | [L3](#) |
|      |       | Lecture 4 - Dependency Parsing |[L4](#) |
|      |       | [A Fast and Accurate Dependency Parser using Neural Networks](https://emnlp2014.org/papers/pdf/EMNLP2014082.pdf) | [Slides](#) |
| 3    | 03/31 | Lecture 5 - Recurrent Neural Network | Link |
|      |       | Lecture 6 - Sequence to Sequence Models | Link |
|      |       | Lecture 7 - Attention & LLM Intro | Link |
|      |       | [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215) | Link |
| 4    | 04/07 | Lecture 8 - Self-Attention & Transformers | Link |
|      |       | Lecture 9 - Pretraining | Link |
|      |       | [Attention Is All You Need](#) | Link |
|      |       | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](#) | Link |
| 5    | 04/08~04/28 | Mid-term Exam  |                |

---
### Study Members
- 민세희	(AI학과)	
- 민유안	(소프트웨어학부)	
- 정인혁	(소프트웨어학부)
- 김지호	(미디어커뮤니케이션학부)	
- 태아카	(소프트웨어학부)
- 양희원	(AI학과)	
